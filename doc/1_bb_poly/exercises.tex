\begin{exercise}
    It is sometimes necessary to convert a polynomial in BB form to monomial form.
    Consider a quadratic BB polynomial,
    \begin{equation*}
        p(x) = c_0 (1 - x)^2 + 2c_1 x(1 - x) + c_2 x^2.
    \end{equation*}
    Express $p$ in the monomial form
    \begin{equation*}
        p(x) = a_0 + a_1 x + a_2 x^2.
    \end{equation*}
\end{exercise}

\begin{solution}
    Rather than using the explicit formula for conversion, we can just expand the coefficients and collect terms.
    \begin{align*}
        p(x) &= c_0 (1 - x)^2 + 2c_1 x(1 - x) + c_2 x^2 \\
        &= c_0 (1 - 2x + x^2) + 2c_1 (x - x^2) + c_2 x^2 \\
        &= c_0 - 2c_0 x + c_0 x^2 + 2c_1 x - 2c_1 x^2 + c_2 x^2 \\
        &= c_0 + (-2c_0 + 2c_1) x + (c_0 - 2c_1 + c_2) x^2.
    \end{align*}
\end{solution}

\begin{exercise}
    Consider a polynomial $p(x)$ of degree $\leq d$, for arbitrary $d$.
    Show that if
    \begin{equation*}
        p(x) = \sum_{j=0}^d a_j x^j = \sum_{i=0}^d c_i B_i^d(x),
    \end{equation*}
    then
    \begin{equation*}
        a_j = \binom{d}{j} \Delta^j c_0.
    \end{equation*}
    \textit{Hint:} Use a Taylor approximation to $p$ to show that $a_j = p^{(j)}(0)/j!$. % chktex 40
\end{exercise}

\begin{solution}
    We have that
    \begin{equation*}
        p(x) = \sum_{j=0}^d a_j x^j = \sum_{i=0}^d c_i B_i^d(x).
    \end{equation*}
    By the Taylor approximation, we have that
    \begin{equation*}
        p(x) = p(x + 0) = \sum_{j=0}^d \frac{p^{(j)}(0)}{j!} x^j.
    \end{equation*}
    We thus have that
    \begin{equation*}
        a_j = \frac{p^{(j)}(0)}{j!}.
    \end{equation*}
    By properties of the Bézier curves, we have that
    \begin{equation*}
        p^{(j)}(x) = \frac{d!}{(d-j)!} \sum_{i = 0}^{d - j} \Delta^j c_i B_i^{d-j}(x),
    \end{equation*}
    and specifically for $x = 0$,
    \begin{equation*}
        p^{(j)}(0) = \frac{d!}{(d-j)!} \Delta^j c_0.
    \end{equation*}
    Combining these results, we have that
    \begin{equation*}
        a_j = \frac{p^{(j)}(0)}{j!} = \frac{d!}{(d-j)! j!} \Delta^j c_0 = \binom{d}{j} \Delta^j c_0,
    \end{equation*}
    as we wanted to show.
\end{solution}

\begin{exercise}
    We might also want to convert a polynomial from monomial form to BB form.
    Using Lemma 1.2, show that in the notation of the previous question,
    \begin{equation*}
        c_i = \frac{i!}{d!} \sum_{j=0}^i \frac{(d - j)!}{(i - j)!} a_j.
    \end{equation*}
\end{exercise}

\begin{solution}
    Lemma~1.2 states that for $j = 0, 1, \ldots, d$,
    \begin{equation*}
        x^j = \frac{(d - j)!}{d!} \sum_{i = j}^d \frac{i!}{(i - j)!} B_i^d(x).
    \end{equation*}

    We have that
    \begin{gather*}
        \sum_{j = 0}^d a_j x^j = \sum_{i = 0}^d c_i B_i^d(x) \\
        \sum_{j = 0}^d a_j \left[
            \frac{(d - j)!}{d!} \sum_{i = j}^d \frac{i!}{(i - j)!} B_i^d(x)
        \right]
        = \sum_{i = 0}^d c_i B_i^d(x) \\
    \end{gather*}
    As we have $i \geq j$, we can reorder the summation to the form $j \leq i$, by using
    \begin{equation*}
        \sum_{j = 0}^d \sum_{i = j}^d (\ldots) = \sum_{i = 0}^d \sum_{j = 0}^i (\ldots).
    \end{equation*}
    This gives us
    \begin{equation*}
        \sum_{i = 0}^d \left[
            \sum_{j = 0}^i a_j \frac{(d - j)!}{d!} \frac{i!}{(i - j)!}
        \right] B_i^d(x) = \sum_{i = 0}^d c_i B_i^d(x).
    \end{equation*}
    Which by isolating the coefficients, gives us
    \begin{equation*}
        c_i = \frac{i!}{d!} \sum_{j = 0}^i \frac{(d - j)!}{(i - j)!} a_j,
    \end{equation*}
    as we wanted to show.
\end{solution}

\begin{exercise}
    Implement the de Casteljau algorithm for cubic Bézier curves in Matlab or Python (or some other programming language), taking repeated convex combinations.
    Choose a sequence of four control points and plot both the control polygon and the Bézier curve, like those in Figure~1.3.
\end{exercise}

\begin{solution}
    The de Casteljau algorithm uses recursion to compute the value of a point along a Bézier curve by the following formula:
    \begin{enumerate}
        \item Initialize by setting $c_i^0 = c_i$ for $i = 0, 1, \ldots, d$.
        \item Then, for each $r = 1, 2, \ldots, d$, let
            \begin{equation*}
                c_i^r = (1 - x) c_i^{r-1} + x c_{i+1}^{r-1},
                \quad i = 0, 1, \ldots, d - r.
            \end{equation*}
        \item The last value $c_0^d$ is the value of the Bézier curve at $x$.
    \end{enumerate}

    This is implemented using Jax in Python in \verb|de_casteljau.py|, and the result is shown in Figure~\ref{fig:de_casteljau}, bearing a striking resemblance to the figure in the book.
    \begin{figure}[!ht]
        \centering
        \includegraphics[width=0.6\textwidth]{1_bb_poly/de_casteljau.pdf}
        \caption{The de Casteljau algorithm applied to a cubic Bézier curve, with control points $(0.2, 0.4, -0.1, 0.5)$, illustrated at the point $x = 0.6$.\label{fig:de_casteljau}}
    \end{figure}
\end{solution}

\begin{exercise}
    Show that the graph, $g(x) = (x, p(x))$ of the BB polynomial $p$ in (1.6) is a Bézier curve in $\mathbb{R}^2$, with control points $(\xi_i, c_i)$, $i = 0, 1, \ldots, d$, where $\xi_i = i/d$.
    \textit{Hint:} Express $x$ as a linear combination of $B_0^d(x), \ldots, B_d^d(x)$.
\end{exercise}

\begin{solution}
    We can again utilize Lemma~1.2 to express $x$ as a linear combination of the Bernstein polynomials.
    We have that, writing $x = x^1$ for clarity,
    \begin{equation*}
        x^1 = \frac{(d - 1)!}{d!} \sum_{i = 1}^d \frac{i!}{(i - 1)!} B_i^d(x) = \sum_{i = 1}^d \frac{i}{d} B_i^d(x) = \sum_{i = 0}^d \frac{i}{d} B_i^d(x) = \sum_{i = 0}^d \xi_i B_i^d(x).
    \end{equation*}
    We can now express the graph of $p$ as a Bézier curve in $\mathbb{R}^2$ by
    \begin{equation*}
        g(x)
        = (x, p(x))
        = \left(
            \sum_{i = 0}^d \xi_i B_i^d(x), \sum_{i = 0}^d c_i B_i^d(x)
        \right)
        = \sum_{i = 0}^d (\xi_i, c_i) B_i^d(x)
        = \sum_{i = 0}^d \boldsymbol{c}_i B_i^d(x),
    \end{equation*}
    where $\boldsymbol{c}_i = (\xi_i, c_i)$ are the control points of the Bézier curve.
\end{solution}

\begin{exercise}
    Show that the tangent vector $\boldsymbol{p}'(x)$ of the Bézier curve in (1.6) lies in the convex cone of the vectors $\Delta \boldsymbol{c}_i$, i.e., in
    \begin{equation*}
        \text{cone}(\Delta \boldsymbol{c}_0, \ldots, \Delta \boldsymbol{c}_{d-1})
        = \left\{
            \sum_{i = 0}^{d-1} \mu_i \Delta \boldsymbol{c}_i
            : \mu_1, \ldots, \mu_{d-1} \geq 0
        \right\}.
    \end{equation*}
\end{exercise}

\begin{solution}
    The derivative (or perhaps \textit{gradient} is the correct term) of the Bézier curve $\boldsymbol{p}(x)$ is given by
    \begin{equation*}
        \boldsymbol{p}'(x)
        = d \sum_{i = 0}^{d-1} \left(
            \boldsymbol{c}_{i+1} - \boldsymbol{c}_i
        \right) B_i^{d-1}(x)
        = d \sum_{i = 0}^{d-1} \Delta \boldsymbol{c}_i B_i^{d-1}(x).
    \end{equation*}
    As $B_i^{d-1}(x) \geq 0$ for $x \in [0, 1]$, we can set $\mu_i = d B_i^{d-1}(x)$, and we have that
    \begin{equation*}
        \boldsymbol{p}'(x) = \sum_{i = 0}^{d-1} \mu_i \Delta \boldsymbol{c}_i \in \text{cone}(\Delta \boldsymbol{c}_0, \ldots, \Delta \boldsymbol{c}_{d-1}),
    \end{equation*}
    as we wanted to show.
\end{solution}

\begin{exercise}
    Show that the first derivative of $p$ in~(1.6) can be expressed (and computed) as
    \begin{equation*}
        p'(x) = d(c_{1}^{d-1} - c_{0}^{d-1}),
    \end{equation*}
    where $c_{1}^{d-1}, c_{0}^{d-1}$ are the points of order $d-1$ in de Casteljau's algorithm~(1.10).
\end{exercise}

\begin{solution}
    We have that
    \begin{equation*}
        p(x) = c_0^d = (1 - x) c_0^{d-1} + x c_1^{d-1},
    \end{equation*}
    and thus by differentiating with respect to $x$, we have that
    \begin{equation*}
        p'(x) = c_1^{d-1} - c_0^{d-1}.
    \end{equation*}
    This tells us that we cannot be as naive as this, as $c_0^d$ is actually a function of $x$, and not simply a constant.

    What we might instead need to note is that
    \begin{equation*}
        c_i^r = \sum_{j = 0}^r c_{i+j} B_j^r(x),
    \end{equation*}
    and combining this with the fact that
    \begin{equation*}
        \left( B_i^d \right)'(x) = d \left( B_{i-1}^{d-1} - B_i^{d-1} \right)(x),
    \end{equation*}
    we have that
    \begin{align*}
        p'(x)
        &= d \sum_{i = 0}^{d-1} \left( c_{i+1} - c_i \right) B_i^{d-1}(x)
        = d \left[
            \sum_{i = 0}^{d-1} c_{i+1} B_i^{d-1}(x) - \sum_{i = 0}^{d-1} c_i B_i^{d-1}(x)
        \right] \\
        &= d(c_{1}^{d-1} - c_{0}^{d-1}),
    \end{align*}
    as we wanted to show.
\end{solution}

\begin{exercise}
    Show that the Bernstein basis polynomial $B_i^d(x)$ has only one maximum in $[0, 1]$, namely at $x = i/d$.
\end{exercise}

\begin{solution}
    We do this by firstly computing the derivative of $B_i^d(x)$, which is given by
    \begin{equation*}
        \left(B_i^d\right)'(x) = d \left( B_{i-1}^{d-1}(x) - B_i^{d-1}(x) \right).
    \end{equation*}
    A maximum or minimum of a function occurs where the derivative is zero, so we set
    \begin{align*}
        B_{i-1}^{d-1}(x) &= B_i^{d-1}(x) \\
        \frac{(d-1)!}{(i-1)!(d-i)!} x^{i-1} (1 - x)^{d-i} &= \frac{(d-1)!}{i!(d-1-i)!} x^i (1 - x)^{d-i-1} \\
        \frac{
            \cancel{(d-1)!} i! (d-1-i)!
        }{(i-1)! (d-i)! \cancel{(d-1)!}} \cancel{x^{i-1}} (1 - x)^{\cancel{d-i}} &= x^{\cancel{i}} \cancel{(1 - x)^{d-i-1}} \\
        \frac{i}{d-i} (1 - x) &= x \\
        i - ix &= dx - ix \\
        x &= \frac{i}{d}.
    \end{align*}
    We have thus shown that the Bernstein basis polynomials only have one extremal point.

    We can use the second derivative to test if this is a maximum or a minimum, however we can instead note that $B_i^d(x)$ is a non-negative polynomial, which is only zero at either $x = 0$ or $x = 1$, and thus $x = i / d$ must be a maximum.
\end{solution}

\begin{exercise}
    Give a proof of the forward difference formula,~(1.15).
\end{exercise}

\begin{solution}
    The forward difference formula~(1.15) is given by
    \begin{equation*}
        \Delta^r c_0 = \sum_{i = 0}^r \binom{r}{i} (-1)^{r-i} c_i.
    \end{equation*}
    The forward difference operator is defined by the recursion
    \begin{equation*}
        \Delta^r c_i = \Delta^{r-1} c_{i+1} - \Delta^{r-1} c_i,
    \end{equation*}
    where $\Delta^0 c_i = c_i$.

    We prove this by induction on $r$.
    For the base case $r = 1$, we have that
    \begin{equation*}
        \Delta c_0 = c_1 - c_0 = \binom{1}{0} (-1)^{1-0} c_0 + \binom{1}{1} (-1)^{1-1} c_1.
    \end{equation*}
    For the induction step, we assume that the formula holds for $r = k$, and show that it holds for $r = k + 1$.
    We have that
    \begin{align*}
        &\Delta^{k+1} c_0 \\
        &= \Delta^k c_1 - \Delta^k c_0 \\
        &= \sum_{i = 0}^k \binom{k}{i} (-1)^{k-i} c_{i+1} - \sum_{i = 0}^k \binom{k}{i} (-1)^{k-i} c_i \\
        &= \sum_{i = 1}^{k+1} \binom{k}{i-1} (-1)^{k-i+1} c_i - \sum_{i = 0}^k \binom{k}{i} (-1)^{k-i} c_i \\
        &= \binom{k}{k} c_{k+1} + \sum_{i = 1}^k \left(
            \binom{k}{i-1} (-1)^{k-i+1} - \binom{k}{i} (-1)^{k-i}
        \right) c_i - \binom{k}{0} (-1)^{k} c_0 \\
        &= \binom{k+1}{k+1} c_{k+1} + \sum_{i = 1}^k (-1)^{(k+1)-i} \left(
            \binom{k}{i-1} + \binom{k}{i}
        \right) c_i + \binom{k+1}{0} (-1)^{k+1} c_0 \\
        &= \sum_{i = 0}^{k+1} \binom{k+1}{i} (-1)^{k+1-i} c_i,
    \end{align*}
    as we wanted to show.
\end{solution}

\begin{exercise}
    The Bernstein approximation to a function $f : [0, 1] \to \mathbb{R}$ of order $d$ is the polynomial $g : [0, 1] \to \mathbb{R}$ defined by
    \begin{equation*}
        g(x) = \sum_{i = 0}^d f\left(\frac{i}{d}\right) B_i^d(x).
    \end{equation*}
    Show that if $f$ is a polynomial of degree $m \leq d$, then $g$ has degree $m$.
\end{exercise}

\begin{solution}
    Let $q$ be the polynomial defined by
    \begin{equation*}
        q(x) = f(x) - g(x).
    \end{equation*}
    We have that $f$ is a polynomial of degree $m \leq d$.
    As
    \begin{equation*}
        q\left(\frac{i}{d}\right) = f\left(\frac{i}{d}\right) - g\left(\frac{i}{d}\right) = 0,
    \end{equation*}
    we have that $q$ has $d+1$ roots, and thus $q$ is either a polynomial of degree $d+1$, or $q = 0$.
    However, as $q$ is the sum of two polynomials of degree $m$ and $d$, respectively, we have that $q$ is a polynomial of degree $\max(m, d)$.
    As $m \leq d$, we have that $q$ is at most a polynomial of degree $d$, and thus $q = 0$.
    $q$ being the zero polynomial implies that $g = f$, and thus $g$ has degree $m$.
\end{solution}

\begin{exercise}
    Show that the length of the Bézier curve $p$ in~(1.9) is bounded by the length of its control polygon,
    \begin{equation*}
        \text{length}(p) \leq \sum_{i = 0}^{d-1} \lVert \Delta \boldsymbol{c}_i \rVert.
    \end{equation*}
\end{exercise}

\begin{solution}
    The length of a curve is given by the integral of the norm of the derivative of the curve, i.e.,
    \begin{equation*}
        \text{length}(p) = \int_0^1 \lVert \boldsymbol{p}'(x) \rVert \, dx.
    \end{equation*}
    We have that
    \begin{equation*}
        \lVert \boldsymbol{p}'(x) \rVert = \lVert d \sum_{i = 0}^{d-1} \Delta \boldsymbol{c}_i B_i^{d-1}(x) \rVert
        = d \lVert \sum_{i = 0}^{d-1} \Delta \boldsymbol{c}_i B_i^{d-1}(x) \rVert
        \leq d \sum_{i = 0}^{d-1} \lVert \Delta \boldsymbol{c}_i \rVert,
    \end{equation*}
    and thus
    \begin{equation*}
        \text{length}(p) = \int_0^1 \lVert \boldsymbol{p}'(x) \rVert \, dx \leq \int_0^1 d \sum_{i = 0}^{d-1} \lVert \Delta \boldsymbol{c}_i \rVert \, dx = \sum_{i = 0}^{d-1} \lVert \Delta \boldsymbol{c}_i \rVert.
    \end{equation*}
\end{solution}